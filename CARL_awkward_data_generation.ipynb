{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#sys.path.append('/home/mdd424/CARL_tthbb/')\n",
    "#sys.path.append('/home/mdd424/downloads/carl-torch')\n",
    "\n",
    "import uproot\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import bisect\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import sigmoid\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchsummary import summary\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import awkward as ak\n",
    "\n",
    "import matplotlib as mpl\n",
    "default_backend = mpl.get_backend()\n",
    "print(default_backend)\n",
    "import matplotlib.pyplot as plt\n",
    "#import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpl.use(default_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jet_features = [\"Jet_pt\", \"Jet_eta\", \"Jet_phi\", \"Jet_mass\"]\n",
    "electron_features = [\"Electron_pt\", \"Electron_eta\", \"Electron_phi\", \"Electron_mass\"]\n",
    "muon_features = [\"Muon_pt\", \"Muon_eta\", \"Muon_phi\", \"Muon_mass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = jet_features + electron_features + muon_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#weight_features = [\"weight_mc\", \"weight_pileup\", \"weight_leptonSF\", \"weight_jvt\", \"weight_bTagSF_DL1r_Continuous\"]\n",
    "weight_features = [\"genWeight\", \"btagWeight_CSVV2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Luminosities in pb^-1\n",
    "luminosities = {'2015': 36207.66, '2017': 44307.4, '2018': 58450.1}\n",
    "luminosities_by_run = {'9364': 36207.66, '10201': 44307.4, '10724': 58450.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"nanoaod_inputs.json\", 'r') as f:\n",
    "    file_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_metadata_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0005.root'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nominal_files = [x[\"path\"] for x in file_dict[\"ttbar\"][\"nominal\"][\"files\"]]\n",
    "print(len(all_nominal_files), all_nominal_files[0])\n",
    "np.random.shuffle(all_nominal_files)\n",
    "all_nominal_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0000.root\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0004.root'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variation_files = [x[\"path\"] for x in file_dict[\"ttbar\"][\"PS_var\"][\"files\"]]\n",
    "print(len(all_variation_files), all_variation_files[0])\n",
    "np.random.shuffle(all_variation_files)\n",
    "all_variation_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal_Nevents = 0\\nvariation_Nevents = 0\\nfor filename in all_nominal_files:\\n    dataset = uproot.open(filename)[\"Events\"].arrays(jet_features[0])\\n    nominal_Nevents += len(dataset)\\nfor filename in all_variation_files:\\n    dataset = uproot.open(filename)[\"Events\"].arrays(jet_features[0])\\n    variation_Nevents += len(dataset)\\nmax_data_index = min([nominal_Nevents, variation_Nevents])\\nmax_data_index'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"nominal_Nevents = 0\n",
    "variation_Nevents = 0\n",
    "for filename in all_nominal_files:\n",
    "    dataset = uproot.open(filename)[\"Events\"].arrays(jet_features[0])\n",
    "    nominal_Nevents += len(dataset)\n",
    "for filename in all_variation_files:\n",
    "    dataset = uproot.open(filename)[\"Events\"].arrays(jet_features[0])\n",
    "    variation_Nevents += len(dataset)\n",
    "max_data_index = min([nominal_Nevents, variation_Nevents])\n",
    "max_data_index\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"carl_data_metadata.json\", 'r') as f:\n",
    "    data_metadata_dict = json.load(f)\n",
    "    max_data_index = data_metadata_dict[\"max_data_index\"]\n",
    "    max_jet_size = data_metadata_dict[\"max_jet_size\"]\n",
    "    max_electron_size = data_metadata_dict[\"max_electron_size\"]\n",
    "    max_muon_size = data_metadata_dict[\"max_muon_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11602238 15469650 19337062\n"
     ]
    }
   ],
   "source": [
    "train_frac = 0.6\n",
    "val_frac = 0.2\n",
    "test_frac = 0.2\n",
    "\n",
    "max_train_index = int(train_frac * max_data_index)\n",
    "max_val_index = max_train_index + int(val_frac * max_data_index)\n",
    "max_test_index = max_val_index + int(test_frac * max_data_index)\n",
    "\n",
    "np.random.shuffle(all_nominal_files)\n",
    "np.random.shuffle(all_variation_files)\n",
    "print(max_train_index, max_val_index, max_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_sizes(filename):\n",
    "    dataset = uproot.open(filename)[\"Events\"].arrays([jet_features[0], electron_features[0], muon_features[0]])\n",
    "    max_jet_size = max(map(len, dataset[jet_features[0]]))\n",
    "    max_electron_size = max(map(len, dataset[electron_features[0]]))\n",
    "    max_muon_size = max(map(len, dataset[muon_features[0]]))\n",
    "    return [max_jet_size, max_electron_size, max_muon_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#max_jet_size = 0\n",
    "#for filename in all_nominal_files + all_variation_files:\n",
    "#    dataset = uproot.open(filename)[\"Events\"].arrays(jet_features)\n",
    "#    max_jet_size = max([max(map(len, dataset[jet_features[0]])), max_jet_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_jet_size = 20\n",
    "max_jet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_or_extend_tree(datafile, tree_dict, treename=\"Events\"):\n",
    "    if datafile.get(treename) is None:\n",
    "        datafile[treename] = tree_dict\n",
    "    else:\n",
    "        datafile[treename].extend(tree_dict)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_data_dict(features, arrays, split_index=None, split_low=False, split_high=False):\n",
    "    if split_low is False and split_high is False:\n",
    "        data_dict = dict(zip(features, ak.unzip(arrays)))\n",
    "    elif split_high is True:\n",
    "        data_dict = dict(zip(features, ak.unzip(arrays[:split_index])))\n",
    "    elif split_low is True:\n",
    "        data_dict = dict(zip(features, ak.unzip(arrays[split_index:])))\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate the nominal datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_40001_0000.root\n",
      "1189800\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_40000_0007.root\n",
      "1181800\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_20000_0007.root\n",
      "612355\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_60000_0006.root\n",
      "1175424\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_70000_0017.root\n",
      "1216146\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_40000_0020.root\n",
      "1194600\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_10000_0007.root\n",
      "1125573\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_00000_0021.root\n",
      "1121184\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_00000_0028.root\n",
      "976997\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_60000_0023.root\n",
      "1332200\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_60000_0015.root\n",
      "1262600\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_60000_0016.root\n",
      "1152675\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_40000_0001.root\n",
      "1207800\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_00000_0025.root\n",
      "1086886\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_00000_0015.root\n",
      "1148800\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0002.root\n",
      "1284200\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_70000_0008.root\n",
      "1181028\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "df_train = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_training_data.root\")\n",
    "df_val = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_validation_data.root\")\n",
    "df_test = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_testing_data.root\")\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "current_index = 0\n",
    "for filename in all_nominal_files:\n",
    "    print(\"Loading file: {}\".format(filename))\n",
    "    # Load the data\n",
    "    nominal_dataset = uproot.open(filename)[\"Events\"]#.arrays(jet_features + electron_features + muon_features + weight_features)\n",
    "    filesize = int(nominal_dataset.arrays(jet_features[0]).type.length)\n",
    "    print(filesize)\n",
    "    for i in range(int(np.ceil(filesize / chunk_size))):\n",
    "        #data_arrays = nominal_dataset.arrays(jet_features + electron_features + muon_features + weight_features,\n",
    "        #                                     entry_start=int(i * chunk_size),\n",
    "        #                                     entry_stop=int((i+1) * chunk_size))\n",
    "\n",
    "        jet_arr = nominal_dataset.arrays(jet_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        electron_arr = nominal_dataset.arrays(electron_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        muon_arr = nominal_dataset.arrays(muon_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        weight_arr = nominal_dataset.arrays(weight_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        # Get the run number\n",
    "        #nominal_file_run_number = str(NOMINAL_FILE_TO_RUN_NUMBER[filename])\n",
    "        # Get the DSID number\n",
    "        #nominal_file_dsid = str(NOMINAL_FILE_TO_DSID[filename])\n",
    "        # Get the luminsotiy, DSID cross section, and per DSID total weighted events\n",
    "        #_scale_factor = luminosities_by_run[nominal_file_run_number] * NOMINAL_XSECTIONS[nominal_file_dsid] / NOMINAL_NORMALIZATIONS[nominal_file_dsid]\n",
    "        _scale_factor = 3378 * 1 / 1\n",
    "        # Extract the combined weight array\n",
    "        _weights = ak.concatenate(ak.unzip(weight_arr[weight_features][:, np.newaxis]), axis=1).to_numpy().prod(axis=1)\n",
    "\n",
    "        nominal_padded_jets = ak.fill_none(ak.pad_none(jet_arr[jet_features], max_jet_size, clip=True), 0, axis=1)\n",
    "        nominal_padded_electrons = ak.fill_none(ak.pad_none(electron_arr[electron_features], max_electron_size, clip=True), 0, axis=1)\n",
    "        nominal_padded_muons = ak.fill_none(ak.pad_none(muon_arr[muon_features], max_muon_size, clip=True), 0, axis=1)\n",
    "\n",
    "        current_data_size = len(_weights)\n",
    "\n",
    "        # put everything in train\n",
    "        if current_index + current_data_size < max_train_index:\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "        # put part in train and the rest in val\n",
    "        elif current_index < max_train_index and current_index + current_data_size < max_val_index:\n",
    "            split_index = max_train_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets, split_index=split_index, split_low=True)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons, split_index=split_index, split_low=True))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons, split_index=split_index, split_low=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put everything into val\n",
    "        elif current_index >= max_train_index and current_index + current_data_size < max_val_index:\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put part in val and the rest in test\n",
    "        elif current_index < max_val_index and current_index + current_data_size < max_test_index:\n",
    "            split_index = max_val_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets, split_index=split_index, split_low=True)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons, split_index=split_index, split_low=True))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons, split_index=split_index, split_low=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put everything into test\n",
    "        elif current_index >= max_val_index and current_index + current_data_size < max_test_index:\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put what's needed into test and ignore the rest\n",
    "        elif current_index < max_test_index and current_index + current_data_size >= max_test_index:\n",
    "            split_index = max_test_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        else:\n",
    "            print(\"Uncaught case:\",\n",
    "                  \"current_index: {}\".format(current_index),\n",
    "                  \"current_data_size: {}\".format(current_data_size),\n",
    "                  \"max_train_index: {}\".format(max_train_index),\n",
    "                  \"max_val_index: {}\".format(max_val_index),\n",
    "                  \"max_test_index: {}\".format(max_test_index), sep='\\n')\n",
    "\n",
    "        current_index += current_data_size\n",
    "        if current_index >= max_test_index:\n",
    "            break\n",
    "    if current_index >= max_test_index:\n",
    "        break\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0006.root\n",
      "1347632\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0013.root\n",
      "1339381\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0007.root\n",
      "1383023\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0004.root\n",
      "1309020\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0010.root\n",
      "1374804\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0009.root\n",
      "1293776\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0000.root\n",
      "1310232\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0008.root\n",
      "1264826\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0003.root\n",
      "1327809\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0002.root\n",
      "1327524\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0005.root\n",
      "1332968\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0001.root\n",
      "1253316\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0012.root\n",
      "1262090\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0014.root\n",
      "913318\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0011.root\n",
      "1297345\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "df_train = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_training_data.root\")\n",
    "df_val = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_validation_data.root\")\n",
    "df_test = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_testing_data.root\")\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "current_index = 0\n",
    "for filename in all_variation_files:\n",
    "    print(\"Loading file: {}\".format(filename))\n",
    "    # Load the data\n",
    "    variation_dataset = uproot.open(filename)[\"Events\"]#.arrays(jet_features + electron_features + muon_features + weight_features)\n",
    "    filesize = int(variation_dataset.arrays(jet_features[0]).type.length)\n",
    "    print(filesize)\n",
    "    for i in range(int(np.ceil(filesize / chunk_size))):\n",
    "        jet_arr = variation_dataset.arrays(jet_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        electron_arr = variation_dataset.arrays(electron_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        muon_arr = variation_dataset.arrays(muon_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        weight_arr = variation_dataset.arrays(weight_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        # Get the run number\n",
    "        #variation_file_run_number = str(VARIATION_FILE_TO_RUN_NUMBER[filename])\n",
    "        # Get the DSID number\n",
    "        #variation_file_dsid = str(VARIATION_FILE_TO_DSID[filename])\n",
    "        # Get the luminsotiy, DSID cross section, and per DSID total weighted events\n",
    "        #_scale_factor = luminosities_by_run[variation_file_run_number] * VARIATION_XSECTIONS[variation_file_dsid] / VARIATION_NORMALIZATIONS[variation_file_dsid]\n",
    "        _scale_factor = 3378 * 1 / 1\n",
    "        # Extract the combined weight array\n",
    "        _weights = ak.concatenate(ak.unzip(weight_arr[weight_features][:, np.newaxis]), axis=1).to_numpy().prod(axis=1)\n",
    "\n",
    "        variation_padded_jets = ak.fill_none(ak.pad_none(jet_arr[jet_features], max_jet_size, clip=True), 0, axis=1)\n",
    "        variation_padded_electrons = ak.fill_none(ak.pad_none(electron_arr[electron_features], max_electron_size, clip=True), 0, axis=1)\n",
    "        variation_padded_muons = ak.fill_none(ak.pad_none(muon_arr[muon_features], max_muon_size, clip=True), 0, axis=1)\n",
    "\n",
    "        current_data_size = len(_weights)\n",
    "\n",
    "        # put everything in train\n",
    "        if current_index + current_data_size < max_train_index:\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "        # put part in train and the rest in val\n",
    "        elif current_index < max_train_index and current_index + current_data_size < max_val_index:\n",
    "            split_index = max_train_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets, split_index=split_index, split_low=True)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons, split_index=split_index, split_low=True))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons, split_index=split_index, split_low=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put everything into val\n",
    "        elif current_index >= max_train_index and current_index + current_data_size < max_val_index:\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put part in val and the rest in test\n",
    "        elif current_index < max_val_index and current_index + current_data_size < max_test_index:\n",
    "            split_index = max_val_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets, split_index=split_index, split_low=True)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons, split_index=split_index, split_low=True))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons, split_index=split_index, split_low=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put everything into test\n",
    "        elif current_index >= max_val_index and current_index + current_data_size < max_test_index:\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put what's needed into test and ignore the rest\n",
    "        elif current_index < max_test_index and current_index + current_data_size >= max_test_index:\n",
    "            split_index = max_test_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        else:\n",
    "            print(\"Uncaught case:\",\n",
    "                  \"current_index: {}\".format(current_index),\n",
    "                  \"current_data_size: {}\".format(current_data_size),\n",
    "                  \"max_train_index: {}\".format(max_train_index),\n",
    "                  \"max_val_index: {}\".format(max_val_index),\n",
    "                  \"max_test_index: {}\".format(max_test_index), sep='\\n')\n",
    "\n",
    "        current_index += current_data_size\n",
    "        if current_index >= max_test_index:\n",
    "            break\n",
    "    if current_index >= max_test_index:\n",
    "        break\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Now create simplified datasets for training the CARL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_features = [jet_features[0] + \"{}\".format(i) for i in range(max_jet_size)]\n",
    "extended_features.extend([jet_features[1] + \"{}\".format(i) for i in range(max_jet_size)])\n",
    "extended_features.extend([jet_features[2] + \"{}\".format(i) for i in range(max_jet_size)])\n",
    "extended_features.extend([jet_features[3] + \"{}\".format(i) for i in range(max_jet_size)])\n",
    "\n",
    "extended_features.extend([electron_features[0] + \"{}\".format(i) for i in range(max_electron_size)])\n",
    "extended_features.extend([electron_features[1] + \"{}\".format(i) for i in range(max_electron_size)])\n",
    "extended_features.extend([electron_features[2] + \"{}\".format(i) for i in range(max_electron_size)])\n",
    "extended_features.extend([electron_features[3] + \"{}\".format(i) for i in range(max_electron_size)])\n",
    "\n",
    "extended_features.extend([muon_features[0] + \"{}\".format(i) for i in range(max_muon_size)])\n",
    "extended_features.extend([muon_features[1] + \"{}\".format(i) for i in range(max_muon_size)])\n",
    "extended_features.extend([muon_features[2] + \"{}\".format(i) for i in range(max_muon_size)])\n",
    "extended_features.extend([muon_features[3] + \"{}\".format(i) for i in range(max_muon_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_tree = uproot.open(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_training_data.root\")[\"Events\"]\n",
    "alternative_tree = uproot.open(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_training_data.root\")[\"Events\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11602238\n"
     ]
    }
   ],
   "source": [
    "df = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_training_data_CARL.root\")\n",
    "chunk_size = 100000\n",
    "\n",
    "# Load the data\n",
    "filesize = int(nominal_tree.arrays(jet_features[0]).type.length)\n",
    "print(filesize)\n",
    "\n",
    "weight_mean = ak.unzip(nominal_tree.arrays([\"weight_mc_combined\"]))[0].to_numpy().mean()\n",
    "for i in range(int(np.ceil(filesize / chunk_size))):\n",
    "    X_array = ak.concatenate(ak.unzip(nominal_tree.arrays(features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))), axis=1).to_numpy().astype(np.float32)\n",
    "    weight_array = ak.unzip(nominal_tree.arrays([\"weight_mc_combined\"], entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size)))[0].to_numpy()\n",
    "    # Normalize by the mean for training stability\n",
    "    weight_array /= weight_mean\n",
    "\n",
    "    data_dict = dict()\n",
    "    for ix, feat in enumerate(extended_features):\n",
    "        data_dict[feat] = X_array[:, ix].ravel()\n",
    "    data_dict[\"weight_mc_combined\"] = weight_array\n",
    "    fill_or_extend_tree(df, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11602238\n"
     ]
    }
   ],
   "source": [
    "df = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_training_data_CARL.root\")\n",
    "chunk_size = 100000\n",
    "\n",
    "# Load the data\n",
    "filesize = int(alternative_tree.arrays(jet_features[0]).type.length)\n",
    "print(filesize)\n",
    "\n",
    "weight_mean = ak.unzip(alternative_tree.arrays([\"weight_mc_combined\"]))[0].to_numpy().mean()\n",
    "for i in range(int(np.ceil(filesize / chunk_size))):\n",
    "    X_array = ak.concatenate(ak.unzip(alternative_tree.arrays(features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))), axis=1).to_numpy().astype(np.float32)\n",
    "    weight_array = ak.unzip(alternative_tree.arrays([\"weight_mc_combined\"], entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size)))[0].to_numpy()\n",
    "    # Normalize by the mean for training stability\n",
    "    weight_array /= weight_mean\n",
    "\n",
    "    data_dict = dict()\n",
    "    for ix, feat in enumerate(extended_features):\n",
    "        data_dict[feat] = X_array[:, ix].ravel()\n",
    "    data_dict[\"weight_mc_combined\"] = weight_array\n",
    "    fill_or_extend_tree(df, data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ------------------------------\n",
    "# Section 2\n",
    "# ------------------------------\n",
    "# Create a dataset using the weight estimators as replacements for the MC weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightRegression(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=3, nodes_per_layer=50, x_scaler=None, y_scaler=None, bandwidth=1):\n",
    "        super(WeightRegression, self).__init__()\n",
    "        \n",
    "        self._x_scaler = x_scaler\n",
    "        self._y_scaler = y_scaler\n",
    "        self._bandwidth = bandwidth\n",
    "        \n",
    "        self._flatten = nn.Flatten()\n",
    "        relu_layers = []\n",
    "        relu_layers.append(nn.Linear(input_dim, nodes_per_layer))\n",
    "        relu_layers.append(nn.ReLU())\n",
    "        for i in range(num_layers-1):\n",
    "            relu_layers.append(nn.Linear(nodes_per_layer, nodes_per_layer))\n",
    "            relu_layers.append(nn.ReLU())\n",
    "        relu_layers.append(nn.Linear(nodes_per_layer, 1)) # output layer -> dim=1 for weight prediction\n",
    "        \n",
    "        self._relu_layers = nn.Sequential(*relu_layers)\n",
    "        \n",
    "        self._loss = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "\n",
    "    def scale_x(self, x, inverse=False):\n",
    "        if self._x_scaler is None:\n",
    "            return x\n",
    "        else:\n",
    "            if inverse is False:\n",
    "                return self._x_scaler.transform(x)\n",
    "            else:\n",
    "                return self._x_scaler.inverse_transform(x)\n",
    "\n",
    "\n",
    "    def scale_y(self, y, inverse=False):\n",
    "        if self._y_scaler is None:\n",
    "            return y\n",
    "        else:\n",
    "            if inverse is False:\n",
    "                return self._y_scaler.transform(y)\n",
    "            else:\n",
    "                return self._y_scaler.inverse_transform(y)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self._flatten(x)\n",
    "        x = self._relu_layers(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def kernel(self, x, y, bandwidth=1):\n",
    "        return torch.exp(-torch.pow(x-y, 2)/(2*bandwidth))   \n",
    "        #return torch.exp(-LA.vector_norm(x-y, dim=1)/(2*self._bandwidth))        \n",
    "    \n",
    "    \n",
    "    def get_MMD(self, x, pred, target):\n",
    "        N = len(target)\n",
    "        #pred = pred.reshape(-1,1)\n",
    "        #target = target.reshape(-1,1)\n",
    "        nn_x = x*pred\n",
    "        mc_x = x*target\n",
    "        \n",
    "        term1 = torch.zeros(1, requires_grad=True)\n",
    "        term2 = torch.zeros(1,requires_grad=True)\n",
    "        term3 = torch.zeros(1, requires_grad=True)\n",
    "        \n",
    "        bandwidths = [0.5, 1, 2, 5, 10]\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                for b in bandwidths:\n",
    "                    term1 = term1 + self.kernel(nn_x[i], nn_x[j], bandwidth=b)\n",
    "                    term2 = term2 + self.kernel(nn_x[i], mc_x[j], bandwidth=b)\n",
    "                    term3 = term3 + self.kernel(mc_x[i], mc_x[j], bandwidth=b)\n",
    "        loss = (term1 + term3 - 2*term2) / N**2\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def get_loss(self, x, pred, target, weights=None):\n",
    "        N = len(target)\n",
    "        #loss = torch.pow(torch.mean(x*pred)- torch.mean(x*target), 2)\n",
    "        #loss = loss + torch.pow(torch.mean(torch.pow(x*pred, 2)) - torch.mean(torch.pow(x*target, 2)), 2)/2\n",
    "        #loss = loss + torch.pow(torch.mean(torch.pow(x*pred, 3)) - torch.mean(torch.pow(x*target, 3)), 2)/6\n",
    "        #return loss\n",
    "        if weights is None:\n",
    "            return self._loss(pred, target).mean()\n",
    "        else:\n",
    "            return (self._loss(pred, target)*weights).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the scaling metadata for these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"PP8_ttbb_weights_jets_model_metadata.json\", 'r') as f:\n",
    "    PP8_weights_model_metadata = json.load(f)\n",
    "    PP8_weights_model_X_scaler = preprocessing.StandardScaler()\n",
    "    PP8_weights_model_y_scaler = preprocessing.StandardScaler()\n",
    "    \n",
    "    PP8_weights_model_X_scaler.scale_ = PP8_weights_model_metadata[\"x scale\"][\"scale\"]\n",
    "    PP8_weights_model_X_scaler.mean_ = PP8_weights_model_metadata[\"x scale\"][\"mean\"]\n",
    "    PP8_weights_model_X_scaler.var_ = PP8_weights_model_metadata[\"x scale\"][\"var\"]\n",
    "    PP8_weights_model_X_scaler.n_features_in_ = PP8_weights_model_metadata[\"x scale\"][\"n_features_in\"]\n",
    "    \n",
    "    PP8_weights_model_y_scaler.scale_ = PP8_weights_model_metadata[\"weight scale\"][\"scale\"]\n",
    "    PP8_weights_model_y_scaler.mean_ = PP8_weights_model_metadata[\"weight scale\"][\"mean\"]\n",
    "    PP8_weights_model_y_scaler.var_ = PP8_weights_model_metadata[\"weight scale\"][\"var\"]\n",
    "    PP8_weights_model_y_scaler.n_features_in_ = PP8_weights_model_metadata[\"weight scale\"][\"n_features_in\"]\n",
    "    \n",
    "with open(\"Sherpa_ttbb_weights_jets_model_metadata.json\", 'r') as f:\n",
    "    Sherpa_weights_model_metadata = json.load(f)\n",
    "    Sherpa_weights_model_X_scaler = preprocessing.StandardScaler()\n",
    "    Sherpa_weights_model_y_scaler = preprocessing.StandardScaler()\n",
    "    \n",
    "    Sherpa_weights_model_X_scaler.scale_ = Sherpa_weights_model_metadata[\"x scale\"][\"scale\"]\n",
    "    Sherpa_weights_model_X_scaler.mean_ = Sherpa_weights_model_metadata[\"x scale\"][\"mean\"]\n",
    "    Sherpa_weights_model_X_scaler.var_ = Sherpa_weights_model_metadata[\"x scale\"][\"var\"]\n",
    "    Sherpa_weights_model_X_scaler.n_features_in_ = Sherpa_weights_model_metadata[\"x scale\"][\"n_features_in\"]\n",
    "    \n",
    "    Sherpa_weights_model_y_scaler.scale_ = Sherpa_weights_model_metadata[\"weight scale\"][\"scale\"]\n",
    "    Sherpa_weights_model_y_scaler.mean_ = Sherpa_weights_model_metadata[\"weight scale\"][\"mean\"]\n",
    "    Sherpa_weights_model_y_scaler.var_ = Sherpa_weights_model_metadata[\"weight scale\"][\"var\"]\n",
    "    Sherpa_weights_model_y_scaler.n_features_in_ = Sherpa_weights_model_metadata[\"weight scale\"][\"n_features_in\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the weight models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeightRegression(\n",
       "  (_flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (_relu_layers): Sequential(\n",
       "    (0): Linear(in_features=80, out_features=200, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=200, out_features=200, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=200, out_features=1, bias=True)\n",
       "  )\n",
       "  (_loss): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PP8_weights_model = WeightRegression(len(PP8_weights_model_metadata[\"features\"]),\n",
    "                                     num_layers=PP8_weights_model_metadata[\"num_layers\"],\n",
    "                                     nodes_per_layer=PP8_weights_model_metadata[\"nodes_per_layer\"])\n",
    "PP8_weights_model.load_state_dict(torch.load(\"PP8_ttbb_weights_jets_model.pt\"))\n",
    "PP8_weights_model.eval()\n",
    "\n",
    "Sherpa_weights_model = WeightRegression(len(Sherpa_weights_model_metadata[\"features\"]),\n",
    "                                        num_layers=Sherpa_weights_model_metadata[\"num_layers\"],\n",
    "                                        nodes_per_layer=Sherpa_weights_model_metadata[\"nodes_per_layer\"])\n",
    "Sherpa_weights_model.load_state_dict(torch.load(\"Sherpa_ttbb_weights_jets_model.pt\"))\n",
    "Sherpa_weights_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the models on the training data to get new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_tree = uproot.open(\"mixture_model_data/PP8_ttbb_full_validation_data.root\")[\"nominal_Loose\"]\n",
    "alternative_tree = uproot.open(\"mixture_model_data/Sherpa_ttbb_full_validation_data.root\")[\"nominal_Loose\"]\n",
    "\n",
    "X_total_datasets = [ak.concatenate(ak.unzip(nominal_tree.arrays(jet_features)), axis=1).to_numpy().astype(np.float32),\n",
    "                    ak.concatenate(ak.unzip(alternative_tree.arrays(jet_features)), axis=1).to_numpy().astype(np.float32)]\n",
    "\n",
    "PP8_X_input = torch.from_numpy(PP8_weights_model_X_scaler.transform(X_total_datasets[0]))\n",
    "Sherpa_X_input = torch.from_numpy(Sherpa_weights_model_X_scaler.transform(X_total_datasets[1]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    PP8_pred_vals = PP8_weights_model(PP8_X_input)\n",
    "    Sherpa_pred_vals = Sherpa_weights_model(Sherpa_X_input)\n",
    "\n",
    "PP8_nn_weights = PP8_weights_model_y_scaler.inverse_transform(PP8_pred_vals.reshape(-1,1))\n",
    "Sherpa_nn_weights = Sherpa_weights_model_y_scaler.inverse_transform(Sherpa_pred_vals.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.00167877]), array([1.0033104]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PP8_nn_weights.mean() / PP8_weights_model_metadata[\"weight scale\"][\"mean\"], Sherpa_nn_weights.mean() / Sherpa_weights_model_metadata[\"weight scale\"][\"mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the new training data with the replacement weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = uproot.recreate(\"mixture_model_data/PP8_ttbb_train_jets_weight_classifier.root\")\n",
    "\n",
    "weight_array = PP8_nn_weights.ravel()\n",
    "# Normalize by the mean for training stability\n",
    "weight_array /= PP8_weights_model_metadata[\"weight scale\"][\"mean\"]\n",
    "\n",
    "data_dict = dict()\n",
    "for ix, feat in enumerate(PP8_weights_model_metadata[\"features\"]):\n",
    "    data_dict[feat] = X_total_datasets[0][:, ix].ravel()\n",
    "data_dict[\"weight_mc_combined\"] = weight_array\n",
    "\n",
    "df[\"nominal_Loose\"] = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = uproot.recreate(\"mixture_model_data/Sherpa_ttbb_train_jets_weight_classifier.root\")\n",
    "\n",
    "weight_array = Sherpa_nn_weights.ravel()\n",
    "# Normalize by the mean for training stability\n",
    "weight_array /= Sherpa_weights_model_metadata[\"weight scale\"][\"mean\"]\n",
    "\n",
    "data_dict = dict()\n",
    "for ix, feat in enumerate(Sherpa_weights_model_metadata[\"features\"]):\n",
    "    data_dict[feat] = X_total_datasets[1][:, ix].ravel()\n",
    "data_dict[\"weight_mc_combined\"] = weight_array\n",
    "\n",
    "df[\"nominal_Loose\"] = data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ============================\n",
    "# Section 3: Make a Deep Sets training dataset\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425208._000008.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000025.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425556._000005.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425211._000003.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425211._000002.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425208._000009.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000032.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425208._000001.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000028.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425059._000008.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000021.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425211._000006.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425211._000005.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425211._000009.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425059._000002.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000019.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425555._000001.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000027.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425208._000004.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425059._000005.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000036.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425208._000010.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425059._000009.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000031.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425208._000006.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000002.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425556._000006.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425555._000002.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425059._000003.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425556._000009.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000026.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000004.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425556._000007.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425555._000004.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000023.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425208._000011.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425556._000008.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425211._000008.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425556._000004.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000035.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425556._000001.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425556._000002.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000003.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425059._000007.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425211._000001.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425556._000003.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425208._000005.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425556._000010.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425059._000004.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425059._000001.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425208._000003.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425208._000012.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425059._000006.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000033.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000020.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425208._000007.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425211._000004.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600791.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_dilep.TOPQ1.e8332s3126r9364p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425555._000003.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425211._000010.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425211._000011.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10201p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425211._000007.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Powheg/semilep/user.aknue.mc16_13TeV.600792.PhPy8EG_NNPDF31ME_ttbb_4FS_pTdef1_ljets.TOPQ1.e8332s3126r10724p4696.TTHbb212206-v1n_1l_out_root/user.aknue.28425062._000022.1l_out.root\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "df_train = uproot.recreate(\"mixture_model_data/PP8_ttbb_full_training_DeepSets_data.root\")\n",
    "df_val = uproot.recreate(\"mixture_model_data/PP8_ttbb_full_validation_DeepSets_data.root\")\n",
    "df_test = uproot.recreate(\"mixture_model_data/PP8_ttbb_full_testing_DeepSets_data.root\")\n",
    "\n",
    "current_index = 0\n",
    "for filename in all_nominal_files:\n",
    "    print(\"Loading file: {}\".format(filename))\n",
    "    # Load the data\n",
    "    nominal_dataset = uproot.open(filename)[\"nominal_Loose\"].arrays(jet_features + weight_features)\n",
    "    # Get the run number\n",
    "    nominal_file_run_number = str(NOMINAL_FILE_TO_RUN_NUMBER[filename])\n",
    "    # Get the DSID number\n",
    "    nominal_file_dsid = str(NOMINAL_FILE_TO_DSID[filename])\n",
    "    # Get the luminsotiy, DSID cross section, and per DSID total weighted events\n",
    "    _scale_factor = luminosities_by_run[nominal_file_run_number] * NOMINAL_XSECTIONS[nominal_file_dsid] / NOMINAL_NORMALIZATIONS[nominal_file_dsid]\n",
    "    # Extract the combined weight array\n",
    "    _weights = ak.concatenate(ak.unzip(nominal_dataset[weight_features][:, np.newaxis]), axis=1).to_numpy().prod(axis=1)\n",
    "    \n",
    "    current_data_size = len(_weights)\n",
    "    \n",
    "    jet_sets = []\n",
    "    for i in range(current_data_size):\n",
    "        jet_sets.append(\n",
    "            np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(nominal_dataset[jet_features][i])], axis=1).flatten()\n",
    "        )\n",
    "    jet_arr = ak.Array(jet_sets)\n",
    "    \n",
    "    # put everything in train\n",
    "    if current_index + current_data_size < max_train_index:\n",
    "        data_dict = {\"jet_4vec\": jet_arr}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "        fill_or_extend_tree(df_train, data_dict)\n",
    "    # put part in train and the rest in val\n",
    "    elif current_index < max_train_index and current_index + current_data_size < max_val_index:\n",
    "        split_index = max_train_index - current_index\n",
    "        data_dict = {\"jet_4vec\": jet_arr[:split_index]}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "        fill_or_extend_tree(df_train, data_dict)\n",
    "        \n",
    "        data_dict = {\"jet_4vec\": jet_arr[split_index:]}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "        fill_or_extend_tree(df_val, data_dict)\n",
    "    # put everything into val\n",
    "    elif current_index >= max_train_index and current_index + current_data_size < max_val_index:\n",
    "        data_dict = {\"jet_4vec\": jet_arr}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "        fill_or_extend_tree(df_val, data_dict)\n",
    "    # put part in val and the rest in test\n",
    "    elif current_index < max_val_index and current_index + current_data_size < max_test_index:\n",
    "        split_index = max_val_index - current_index\n",
    "        data_dict = {\"jet_4vec\": jet_arr[:split_index]}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "        #fill_or_extend_tree(df_val, data_dict)\n",
    "        \n",
    "        data_dict = {\"jet_4vec\": jet_arr[split_index:]}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "        fill_or_extend_tree(df_test, data_dict)\n",
    "    # put everything into test\n",
    "    elif current_index >= max_val_index and current_index + current_data_size < max_test_index:\n",
    "        data_dict = {\"jet_4vec\": jet_arr}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "        fill_or_extend_tree(df_test, data_dict)\n",
    "    # put what's needed into test and ignore the rest\n",
    "    elif current_index < max_test_index and current_index + current_data_size >= max_test_index:\n",
    "        split_index = max_test_index - current_index\n",
    "        data_dict = {\"jet_4vec\": jet_arr[:split_index]}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "        fill_or_extend_tree(df_test, data_dict)\n",
    "    else:\n",
    "        print(\"Uncaught case:\",\n",
    "              \"current_index: {}\".format(current_index),\n",
    "              \"current_data_size: {}\".format(current_data_size),\n",
    "              \"max_train_index: {}\".format(max_train_index),\n",
    "              \"max_val_index: {}\".format(max_val_index),\n",
    "              \"max_test_index: {}\".format(max_test_index), sep='\\n')\n",
    "    \n",
    "    current_index += current_data_size\n",
    "    if current_index >= max_test_index:\n",
    "        break\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403455._000023.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000039.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000023.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000007.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000025.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403455._000010.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000002.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403759._000002.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403097._000014.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700167.Sh_2210_ttbb_dil.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403459._000005.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403455._000002.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403455._000019.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700167.Sh_2210_ttbb_dil.TOPQ1.e8263a875r10201p4434.TTHbb212206-v2n_1l_out_root/user.alheld.28488508._000004.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403759._000013.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403084._000002.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403759._000008.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403097._000002.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000017.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000008.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000013.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403097._000006.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000038.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403759._000017.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403455._000008.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403084._000009.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000027.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403097._000001.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403455._000006.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403084._000003.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403084._000005.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000023.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403097._000009.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403097._000022.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700167.Sh_2210_ttbb_dil.TOPQ1.e8263a875r10201p4434.TTHbb212206-v2n_1l_out_root/user.alheld.28488508._000006.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403084._000011.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000003.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000001.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403759._000005.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000031.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403759._000018.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000034.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700167.Sh_2210_ttbb_dil.TOPQ1.e8263a875r10201p4434.TTHbb212206-v2n_1l_out_root/user.alheld.28488508._000003.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403084._000012.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000036.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000021.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700167.Sh_2210_ttbb_dil.TOPQ1.e8263a875r10201p4434.TTHbb212206-v2n_1l_out_root/user.alheld.28488508._000005.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403455._000012.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000006.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000027.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000020.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700167.Sh_2210_ttbb_dil.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403459._000001.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403084._000010.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700167.Sh_2210_ttbb_dil.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403459._000006.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403759._000011.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000004.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000030.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403455._000018.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403097._000018.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403457._000026.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403084._000006.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700167.Sh_2210_ttbb_dil.TOPQ1.e8263a875r9364p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403111._000009.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000031.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700165.Sh_2210_ttbb_ljetsP.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403455._000017.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700166.Sh_2210_ttbbljetsM.TOPQ1.e8263a875r10201p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403761._000017.1l_out.root\n",
      "Loading file: /scratch/mdd424/CARL_tthbb/data/Sherpa/semilep/user.alheld.mc16_13TeV.700167.Sh_2210_ttbb_dil.TOPQ1.e8263a875r10724p4434.TTHbb212206-v1n_1l_out_root/user.alheld.28403459._000013.1l_out.root\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "df_train = uproot.recreate(\"mixture_model_data/Sherpa_ttbb_full_training_DeepSets_data.root\")\n",
    "df_val = uproot.recreate(\"mixture_model_data/Sherpa_ttbb_full_validation_DeepSets_data.root\")\n",
    "df_test = uproot.recreate(\"mixture_model_data/Sherpa_ttbb_full_testing_DeepSets_data.root\")\n",
    "\n",
    "current_index = 0\n",
    "for filename in all_variation_files:\n",
    "    print(\"Loading file: {}\".format(filename))\n",
    "    # Load the data\n",
    "    variation_dataset = uproot.open(filename)[\"nominal_Loose\"].arrays(jet_features + weight_features)\n",
    "    # Get the run number\n",
    "    variation_file_run_number = str(VARIATION_FILE_TO_RUN_NUMBER[filename])\n",
    "    # Get the DSID number\n",
    "    variation_file_dsid = str(VARIATION_FILE_TO_DSID[filename])\n",
    "    # Get the luminsotiy, DSID cross section, and per DSID total weighted events\n",
    "    _scale_factor = luminosities_by_run[variation_file_run_number] * VARIATION_XSECTIONS[variation_file_dsid] / VARIATION_NORMALIZATIONS[variation_file_dsid]\n",
    "    # Extract the combined weight array\n",
    "    _weights = ak.concatenate(ak.unzip(variation_dataset[weight_features][:, np.newaxis]), axis=1).to_numpy().prod(axis=1)\n",
    "    \n",
    "    current_data_size = len(_weights)\n",
    "    \n",
    "    jet_sets = []\n",
    "    for i in range(current_data_size):\n",
    "        jet_sets.append(\n",
    "            np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(variation_dataset[jet_features][i])], axis=1).flatten()\n",
    "        )\n",
    "    jet_arr = ak.Array(jet_sets)\n",
    "    \n",
    "    # put everything in train\n",
    "    if current_index + current_data_size < max_train_index:\n",
    "        data_dict = {\"jet_4vec\": jet_arr}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "        fill_or_extend_tree(df_train, data_dict)\n",
    "    # put part in train and the rest in val\n",
    "    elif current_index < max_train_index and current_index + current_data_size < max_val_index:\n",
    "        split_index = max_train_index - current_index\n",
    "        data_dict = {\"jet_4vec\": jet_arr[:split_index]}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "        fill_or_extend_tree(df_train, data_dict)\n",
    "        \n",
    "        data_dict = {\"jet_4vec\": jet_arr[split_index:]}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "        fill_or_extend_tree(df_val, data_dict)\n",
    "    # put everything into val\n",
    "    elif current_index >= max_train_index and current_index + current_data_size < max_val_index:\n",
    "        data_dict = {\"jet_4vec\": jet_arr}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "        fill_or_extend_tree(df_val, data_dict)\n",
    "    # put part in val and the rest in test\n",
    "    elif current_index < max_val_index and current_index + current_data_size < max_test_index:\n",
    "        split_index = max_val_index - current_index\n",
    "        data_dict = {\"jet_4vec\": jet_arr[:split_index]}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "        #fill_or_extend_tree(df_val, data_dict)\n",
    "        \n",
    "        data_dict = {\"jet_4vec\": jet_arr[split_index:]}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "        fill_or_extend_tree(df_test, data_dict)\n",
    "    # put everything into test\n",
    "    elif current_index >= max_val_index and current_index + current_data_size < max_test_index:\n",
    "        data_dict = {\"jet_4vec\": jet_arr}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "        fill_or_extend_tree(df_test, data_dict)\n",
    "    # put what's needed into test and ignore the rest\n",
    "    elif current_index < max_test_index and current_index + current_data_size >= max_test_index:\n",
    "        split_index = max_test_index - current_index\n",
    "        data_dict = {\"jet_4vec\": jet_arr[:split_index]}\n",
    "        data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "        fill_or_extend_tree(df_test, data_dict)\n",
    "    else:\n",
    "        print(\"Uncaught case:\",\n",
    "              \"current_index: {}\".format(current_index),\n",
    "              \"current_data_size: {}\".format(current_data_size),\n",
    "              \"max_train_index: {}\".format(max_train_index),\n",
    "              \"max_val_index: {}\".format(max_val_index),\n",
    "              \"max_test_index: {}\".format(max_test_index), sep='\\n')\n",
    "    \n",
    "    current_index += current_data_size\n",
    "    if current_index >= max_test_index:\n",
    "        break\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jet_4vec': <Array [[[1.13e+05, -0.753, ... 8.41e+04]]] type='244098 * var * var * float64'>,\n",
       " 'weight_mc_combined': array([ 0.00393443,  0.00015745,  0.00444214, ..., -0.00029049,\n",
       "         0.00704612,  0.00770962], dtype=float32)}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.48 ms ± 60.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ak.concatenate(ak.unzip(nominal_dataset[jet_features][0][np.newaxis]), axis=0).to_numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ak.concatenate([x[:, np.newaxis] for x in ak.unzip(nominal_dataset[jet_features][0])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362 µs ± 3.51 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(nominal_dataset[jet_features][0])], axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 µs ± 2.28 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.concatenate([np.asarray(x)[:, np.newaxis] for x in ak.unzip(nominal_dataset[jet_features][0])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Section 4: Making a *NEW* DeepSets Dataset\n",
    "\n",
    "Best to have this as separate scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_DeepSets_training_data.root\")\n",
    "df_val = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_DeepSets_validation_data.root\")\n",
    "df_test = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_DeepSets_testing_data.root\")\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "current_index = 0\n",
    "for filename in all_nominal_files:\n",
    "    print(\"Loading file: {}\".format(filename))\n",
    "    # Load the data\n",
    "    nominal_dataset = uproot.open(filename)[\"Events\"]\n",
    "    filesize = int(nominal_dataset.arrays(jet_features[0]).type.length)\n",
    "    print(filesize)\n",
    "    for i in tqdm(range(int(np.ceil(filesize / chunk_size)))):\n",
    "        jet_arr = nominal_dataset.arrays(jet_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        electron_arr = nominal_dataset.arrays(electron_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        muon_arr = nominal_dataset.arrays(muon_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        weight_arr = nominal_dataset.arrays(weight_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        # Get the run number\n",
    "        #nominal_file_run_number = str(NOMINAL_FILE_TO_RUN_NUMBER[filename])\n",
    "        # Get the DSID number\n",
    "        #nominal_file_dsid = str(NOMINAL_FILE_TO_DSID[filename])\n",
    "        # Get the luminsotiy, DSID cross section, and per DSID total weighted events\n",
    "        #_scale_factor = luminosities_by_run[nominal_file_run_number] * NOMINAL_XSECTIONS[nominal_file_dsid] / NOMINAL_NORMALIZATIONS[nominal_file_dsid]\n",
    "        _scale_factor = 3378 * 1 / 1\n",
    "        # Extract the combined weight array\n",
    "        _weights = ak.concatenate(ak.unzip(weight_arr[weight_features][:, np.newaxis]), axis=1).to_numpy().prod(axis=1)\n",
    "\n",
    "        current_data_size = len(_weights)\n",
    "        \n",
    "        jet_sets = []\n",
    "        electron_sets = []\n",
    "        muon_sets = []\n",
    "        for i in range(current_data_size):\n",
    "            jet_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(jet_arr[jet_features][i])], axis=1).flatten()\n",
    "            )\n",
    "            electron_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(electron_arr[electron_features][i])], axis=1).flatten()\n",
    "            )\n",
    "            muon_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(muon_arr[muon_features][i])], axis=1).flatten()\n",
    "            )\n",
    "\n",
    "        nominal_features = [\"jet_4vec\", \"electron_4vec\", \"muon_4vec\"]\n",
    "        nominal_arr = ak.Array({\"jet_4vec\": jet_sets,\n",
    "                                \"electron_4vec\": electron_sets,\n",
    "                                \"muon_4vec\": muon_sets})\n",
    "\n",
    "        # put everything in train\n",
    "        if current_index + current_data_size < max_train_index:\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "        # put part in train and the rest in val\n",
    "        elif current_index < max_train_index and current_index + current_data_size < max_val_index:\n",
    "            split_index = max_train_index - current_index\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr, split_index=split_index, split_low=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put everything into val\n",
    "        elif current_index >= max_train_index and current_index + current_data_size < max_val_index:\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put part in val and the rest in test\n",
    "        elif current_index < max_val_index and current_index + current_data_size < max_test_index:\n",
    "            split_index = max_val_index - current_index\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr, split_index=split_index, split_low=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put everything into test\n",
    "        elif current_index >= max_val_index and current_index + current_data_size < max_test_index:\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put what's needed into test and ignore the rest\n",
    "        elif current_index < max_test_index and current_index + current_data_size >= max_test_index:\n",
    "            split_index = max_test_index - current_index\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        else:\n",
    "            print(\"Uncaught case:\",\n",
    "                  \"current_index: {}\".format(current_index),\n",
    "                  \"current_data_size: {}\".format(current_data_size),\n",
    "                  \"max_train_index: {}\".format(max_train_index),\n",
    "                  \"max_val_index: {}\".format(max_val_index),\n",
    "                  \"max_test_index: {}\".format(max_test_index), sep='\\n')\n",
    "\n",
    "        current_index += current_data_size\n",
    "        if current_index >= max_test_index:\n",
    "            break\n",
    "    if current_index >= max_test_index:\n",
    "        break\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0011.root\n",
      "1297345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3706270514124376a24531e92d1fa776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0001.root\n",
      "1253316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9892ab7a34f401d9f9469d5d389b144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 42\u001b[0m\n\u001b[1;32m     35\u001b[0m     jet_sets\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     36\u001b[0m         np\u001b[38;5;241m.\u001b[39mconcatenate([x\u001b[38;5;241m.\u001b[39mto_numpy()[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ak\u001b[38;5;241m.\u001b[39munzip(jet_arr[jet_features][i])], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m     electron_sets\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     39\u001b[0m         np\u001b[38;5;241m.\u001b[39mconcatenate([x\u001b[38;5;241m.\u001b[39mto_numpy()[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ak\u001b[38;5;241m.\u001b[39munzip(electron_arr[electron_features][i])], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m     muon_sets\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 42\u001b[0m         np\u001b[38;5;241m.\u001b[39mconcatenate([x\u001b[38;5;241m.\u001b[39mto_numpy()[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ak\u001b[38;5;241m.\u001b[39munzip(muon_arr[muon_features][i])], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m variation_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melectron_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmuon_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     46\u001b[0m variation_arr \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mArray({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m: jet_sets,\n\u001b[1;32m     47\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melectron_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m: electron_sets,\n\u001b[1;32m     48\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmuon_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m: muon_sets})\n",
      "Cell \u001b[0;32mIn[20], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m     jet_sets\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     36\u001b[0m         np\u001b[38;5;241m.\u001b[39mconcatenate([x\u001b[38;5;241m.\u001b[39mto_numpy()[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ak\u001b[38;5;241m.\u001b[39munzip(jet_arr[jet_features][i])], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m     electron_sets\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     39\u001b[0m         np\u001b[38;5;241m.\u001b[39mconcatenate([x\u001b[38;5;241m.\u001b[39mto_numpy()[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ak\u001b[38;5;241m.\u001b[39munzip(electron_arr[electron_features][i])], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m     muon_sets\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 42\u001b[0m         np\u001b[38;5;241m.\u001b[39mconcatenate([x\u001b[38;5;241m.\u001b[39mto_numpy()[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ak\u001b[38;5;241m.\u001b[39munzip(muon_arr[muon_features][i])], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m variation_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melectron_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmuon_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     46\u001b[0m variation_arr \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mArray({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjet_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m: jet_sets,\n\u001b[1;32m     47\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melectron_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m: electron_sets,\n\u001b[1;32m     48\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmuon_4vec\u001b[39m\u001b[38;5;124m\"\u001b[39m: muon_sets})\n",
      "File \u001b[0;32m~/miniconda3/envs/carl-for-agc/lib/python3.11/site-packages/awkward/highlevel.py:372\u001b[0m, in \u001b[0;36mArray.to_numpy\u001b[0;34m(self, allow_missing)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m, allow_missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    369\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    Converts this Array into a NumPy array, if possible; same as #ak.to_numpy.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ak\u001b[38;5;241m.\u001b[39moperations\u001b[38;5;241m.\u001b[39mto_numpy(\u001b[38;5;28mself\u001b[39m, allow_missing\u001b[38;5;241m=\u001b[39mallow_missing)\n",
      "File \u001b[0;32m~/miniconda3/envs/carl-for-agc/lib/python3.11/site-packages/awkward/_dispatch.py:43\u001b[0m, in \u001b[0;36mhigh_level_function.<locals>.dispatch\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Failed to find a custom overload, so resume the original function\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28mnext\u001b[39m(gen_or_result)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m err\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniconda3/envs/carl-for-agc/lib/python3.11/site-packages/awkward/operations/ak_to_numpy.py:44\u001b[0m, in \u001b[0;36mto_numpy\u001b[0;34m(array, allow_missing)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m (array,)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Implementation\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _impl(array, allow_missing)\n",
      "File \u001b[0;32m~/miniconda3/envs/carl-for-agc/lib/python3.11/site-packages/awkward/operations/ak_to_numpy.py:51\u001b[0m, in \u001b[0;36m_impl\u001b[0;34m(array, allow_missing)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m  \u001b[38;5;66;03m# noqa: TID251\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m     layout \u001b[38;5;241m=\u001b[39m ak\u001b[38;5;241m.\u001b[39mto_layout(array, allow_record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m     backend \u001b[38;5;241m=\u001b[39m NumpyBackend\u001b[38;5;241m.\u001b[39minstance()\n\u001b[1;32m     54\u001b[0m     numpy_layout \u001b[38;5;241m=\u001b[39m layout\u001b[38;5;241m.\u001b[39mto_backend(backend)\n",
      "File \u001b[0;32m~/miniconda3/envs/carl-for-agc/lib/python3.11/site-packages/awkward/_dispatch.py:19\u001b[0m, in \u001b[0;36mhigh_level_function.<locals>.dispatch\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# NOTE: this decorator assumes that the operation is exposed under `ak.`\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m OperationErrorContext(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mak.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, args, kwargs):\n\u001b[1;32m     20\u001b[0m         gen_or_result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m isgenerator(gen_or_result):\n",
      "File \u001b[0;32m~/miniconda3/envs/carl-for-agc/lib/python3.11/site-packages/awkward/_errors.py:211\u001b[0m, in \u001b[0;36mOperationErrorContext.__init__\u001b[0;34m(self, name, args, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, args: Iterable[Any], kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any]):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39many_backend_is_delayed(args)\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39many_backend_is_delayed(kwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    214\u001b[0m     ):\n\u001b[1;32m    215\u001b[0m         string_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_args(args)\n\u001b[1;32m    216\u001b[0m         string_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/carl-for-agc/lib/python3.11/site-packages/awkward/_errors.py:45\u001b[0m, in \u001b[0;36mErrorContext.primary\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorContext\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Any other threads should get a completely independent _slate.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     _slate \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprimary\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_slate\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__primary_context__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_train = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_DeepSets_training_data.root\")\n",
    "df_val = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_DeepSets_validation_data.root\")\n",
    "df_test = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_DeepSets_testing_data.root\")\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "current_index = 0\n",
    "for filename in all_variation_files:\n",
    "    print(\"Loading file: {}\".format(filename))\n",
    "    # Load the data\n",
    "    variation_dataset = uproot.open(filename)[\"Events\"]\n",
    "    filesize = int(variation_dataset.arrays(jet_features[0]).type.length)\n",
    "    print(filesize)\n",
    "    for i in tqdm(range(int(np.ceil(filesize / chunk_size)))):\n",
    "        jet_arr = variation_dataset.arrays(jet_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        electron_arr = variation_dataset.arrays(electron_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        muon_arr = variation_dataset.arrays(muon_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        weight_arr = variation_dataset.arrays(weight_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        # Get the run number\n",
    "        #variation_file_run_number = str(VARIATION_FILE_TO_RUN_NUMBER[filename])\n",
    "        # Get the DSID number\n",
    "        #variation_file_dsid = str(VARIATION_FILE_TO_DSID[filename])\n",
    "        # Get the luminsotiy, DSID cross section, and per DSID total weighted events\n",
    "        #_scale_factor = luminosities_by_run[variation_file_run_number] * VARIATION_XSECTIONS[variation_file_dsid] / VARIATION_NORMALIZATIONS[variation_file_dsid]\n",
    "        _scale_factor = 3378 * 1 / 1\n",
    "        # Extract the combined weight array\n",
    "        _weights = ak.concatenate(ak.unzip(weight_arr[weight_features][:, np.newaxis]), axis=1).to_numpy().prod(axis=1)\n",
    "\n",
    "        current_data_size = len(_weights)\n",
    "        \n",
    "        jet_sets = []\n",
    "        electron_sets = []\n",
    "        muon_sets = []\n",
    "        for i in range(current_data_size):\n",
    "            jet_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(jet_arr[jet_features][i])], axis=1).flatten()\n",
    "            )\n",
    "            electron_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(electron_arr[electron_features][i])], axis=1).flatten()\n",
    "            )\n",
    "            muon_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(muon_arr[muon_features][i])], axis=1).flatten()\n",
    "            )\n",
    "\n",
    "        variation_features = [\"jet_4vec\", \"electron_4vec\", \"muon_4vec\"]\n",
    "        variation_arr = ak.Array({\"jet_4vec\": jet_sets,\n",
    "                                \"electron_4vec\": electron_sets,\n",
    "                                \"muon_4vec\": muon_sets})\n",
    "\n",
    "        # put everything in train\n",
    "        if current_index + current_data_size < max_train_index:\n",
    "            data_dict = build_data_dict(variation_features, variation_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "        # put part in train and the rest in val\n",
    "        elif current_index < max_train_index and current_index + current_data_size < max_val_index:\n",
    "            split_index = max_train_index - current_index\n",
    "            data_dict = build_data_dict(variation_features, variation_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(variation_features, variation_arr, split_index=split_index, split_low=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put everything into val\n",
    "        elif current_index >= max_train_index and current_index + current_data_size < max_val_index:\n",
    "            data_dict = build_data_dict(variation_features, variation_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put part in val and the rest in test\n",
    "        elif current_index < max_val_index and current_index + current_data_size < max_test_index:\n",
    "            split_index = max_val_index - current_index\n",
    "            data_dict = build_data_dict(variation_features, variation_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(variation_features, variation_arr, split_index=split_index, split_low=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put everything into test\n",
    "        elif current_index >= max_val_index and current_index + current_data_size < max_test_index:\n",
    "            data_dict = build_data_dict(variation_features, variation_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put what's needed into test and ignore the rest\n",
    "        elif current_index < max_test_index and current_index + current_data_size >= max_test_index:\n",
    "            split_index = max_test_index - current_index\n",
    "            data_dict = build_data_dict(variation_features, variation_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        else:\n",
    "            print(\"Uncaught case:\",\n",
    "                  \"current_index: {}\".format(current_index),\n",
    "                  \"current_data_size: {}\".format(current_data_size),\n",
    "                  \"max_train_index: {}\".format(max_train_index),\n",
    "                  \"max_val_index: {}\".format(max_val_index),\n",
    "                  \"max_test_index: {}\".format(max_test_index), sep='\\n')\n",
    "\n",
    "        current_index += current_data_size\n",
    "        if current_index >= max_test_index:\n",
    "            break\n",
    "    if current_index >= max_test_index:\n",
    "        break\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carl-for-agc",
   "language": "python",
   "name": "carl-for-agc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
