{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#sys.path.append('/home/mdd424/CARL_tthbb/')\n",
    "#sys.path.append('/home/mdd424/downloads/carl-torch')\n",
    "\n",
    "import uproot\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import bisect\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import sigmoid\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchsummary import summary\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import awkward as ak\n",
    "\n",
    "import matplotlib as mpl\n",
    "default_backend = mpl.get_backend()\n",
    "print(default_backend)\n",
    "import matplotlib.pyplot as plt\n",
    "#import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpl.use(default_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jet_features = [\"Jet_pt\", \"Jet_eta\", \"Jet_phi\", \"Jet_mass\"]\n",
    "electron_features = [\"Electron_pt\", \"Electron_eta\", \"Electron_phi\", \"Electron_mass\"]\n",
    "muon_features = [\"Muon_pt\", \"Muon_eta\", \"Muon_phi\", \"Muon_mass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = jet_features + electron_features + muon_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#weight_features = [\"weight_mc\", \"weight_pileup\", \"weight_leptonSF\", \"weight_jvt\", \"weight_bTagSF_DL1r_Continuous\"]\n",
    "weight_features = [\"genWeight\", \"btagWeight_CSVV2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Luminosities in pb^-1\n",
    "luminosities = {'2015': 36207.66, '2017': 44307.4, '2018': 58450.1}\n",
    "luminosities_by_run = {'9364': 36207.66, '10201': 44307.4, '10724': 58450.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"nanoaod_inputs.json\", 'r') as f:\n",
    "    file_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0005.root'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nominal_files = [x[\"path\"] for x in file_dict[\"ttbar\"][\"nominal\"][\"files\"]]\n",
    "print(len(all_nominal_files), all_nominal_files[0])\n",
    "np.random.shuffle(all_nominal_files)\n",
    "all_nominal_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0000.root\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0004.root'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variation_files = [x[\"path\"] for x in file_dict[\"ttbar\"][\"PS_var\"][\"files\"]]\n",
    "print(len(all_variation_files), all_variation_files[0])\n",
    "np.random.shuffle(all_variation_files)\n",
    "all_variation_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal_Nevents = 0\\nvariation_Nevents = 0\\nfor filename in all_nominal_files:\\n    dataset = uproot.open(filename)[\"Events\"].arrays(jet_features[0])\\n    nominal_Nevents += len(dataset)\\nfor filename in all_variation_files:\\n    dataset = uproot.open(filename)[\"Events\"].arrays(jet_features[0])\\n    variation_Nevents += len(dataset)\\nmax_data_index = min([nominal_Nevents, variation_Nevents])\\nmax_data_index'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"nominal_Nevents = 0\n",
    "variation_Nevents = 0\n",
    "for filename in all_nominal_files:\n",
    "    dataset = uproot.open(filename)[\"Events\"].arrays(jet_features[0])\n",
    "    nominal_Nevents += len(dataset)\n",
    "for filename in all_variation_files:\n",
    "    dataset = uproot.open(filename)[\"Events\"].arrays(jet_features[0])\n",
    "    variation_Nevents += len(dataset)\n",
    "max_data_index = min([nominal_Nevents, variation_Nevents])\n",
    "max_data_index\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_metadata_dict = {}\n",
    "with open(\"carl_data_metadata.json\", 'r') as f:\n",
    "    data_metadata_dict = json.load(f)\n",
    "    max_data_index = data_metadata_dict[\"max_data_index\"]\n",
    "    max_jet_size = data_metadata_dict[\"max_jet_size\"]\n",
    "    max_electron_size = data_metadata_dict[\"max_electron_size\"]\n",
    "    max_muon_size = data_metadata_dict[\"max_muon_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11602238 15469650 19337062\n"
     ]
    }
   ],
   "source": [
    "train_frac = 0.6\n",
    "val_frac = 0.2\n",
    "test_frac = 0.2\n",
    "\n",
    "max_train_index = int(train_frac * max_data_index)\n",
    "max_val_index = max_train_index + int(val_frac * max_data_index)\n",
    "max_test_index = max_val_index + int(test_frac * max_data_index)\n",
    "\n",
    "np.random.shuffle(all_nominal_files)\n",
    "np.random.shuffle(all_variation_files)\n",
    "print(max_train_index, max_val_index, max_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_sizes(filename):\n",
    "    dataset = uproot.open(filename)[\"Events\"].arrays([jet_features[0], electron_features[0], muon_features[0]])\n",
    "    max_jet_size = max(map(len, dataset[jet_features[0]]))\n",
    "    max_electron_size = max(map(len, dataset[electron_features[0]]))\n",
    "    max_muon_size = max(map(len, dataset[muon_features[0]]))\n",
    "    return [max_jet_size, max_electron_size, max_muon_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#max_jet_size = 0\n",
    "#for filename in all_nominal_files + all_variation_files:\n",
    "#    dataset = uproot.open(filename)[\"Events\"].arrays(jet_features)\n",
    "#    max_jet_size = max([max(map(len, dataset[jet_features[0]])), max_jet_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_jet_size = 20\n",
    "max_jet_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_or_extend_tree(datafile, tree_dict, treename=\"Events\"):\n",
    "    if datafile.get(treename) is None:\n",
    "        datafile[treename] = tree_dict\n",
    "    else:\n",
    "        datafile[treename].extend(tree_dict)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_data_dict(features, arrays, split_index=None, split_low=False, split_high=False):\n",
    "    if split_low is False and split_high is False:\n",
    "        data_dict = dict(zip(features, ak.unzip(arrays)))\n",
    "    elif split_high is True:\n",
    "        data_dict = dict(zip(features, ak.unzip(arrays[:split_index])))\n",
    "    elif split_low is True:\n",
    "        data_dict = dict(zip(features, ak.unzip(arrays[split_index:])))\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Create a standard dataset with zero padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate the nominal datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_40001_0000.root\n",
      "1189800\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_40000_0007.root\n",
      "1181800\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_20000_0007.root\n",
      "612355\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_60000_0006.root\n",
      "1175424\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_70000_0017.root\n",
      "1216146\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_40000_0020.root\n",
      "1194600\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_10000_0007.root\n",
      "1125573\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_00000_0021.root\n",
      "1121184\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_00000_0028.root\n",
      "976997\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_60000_0023.root\n",
      "1332200\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_60000_0015.root\n",
      "1262600\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_60000_0016.root\n",
      "1152675\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_40000_0001.root\n",
      "1207800\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_00000_0025.root\n",
      "1086886\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_00000_0015.root\n",
      "1148800\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19981_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext4-v1_80000_0002.root\n",
      "1284200\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_70000_0008.root\n",
      "1181028\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "df_train = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_training_data.root\")\n",
    "df_val = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_validation_data.root\")\n",
    "df_test = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_testing_data.root\")\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "current_index = 0\n",
    "for filename in all_nominal_files:\n",
    "    print(\"Loading file: {}\".format(filename))\n",
    "    # Load the data\n",
    "    nominal_dataset = uproot.open(filename)[\"Events\"]#.arrays(jet_features + electron_features + muon_features + weight_features)\n",
    "    filesize = int(nominal_dataset.arrays(jet_features[0]).type.length)\n",
    "    print(filesize)\n",
    "    for i in range(int(np.ceil(filesize / chunk_size))):\n",
    "        #data_arrays = nominal_dataset.arrays(jet_features + electron_features + muon_features + weight_features,\n",
    "        #                                     entry_start=int(i * chunk_size),\n",
    "        #                                     entry_stop=int((i+1) * chunk_size))\n",
    "\n",
    "        jet_arr = nominal_dataset.arrays(jet_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        electron_arr = nominal_dataset.arrays(electron_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        muon_arr = nominal_dataset.arrays(muon_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        weight_arr = nominal_dataset.arrays(weight_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        # Get the run number\n",
    "        #nominal_file_run_number = str(NOMINAL_FILE_TO_RUN_NUMBER[filename])\n",
    "        # Get the DSID number\n",
    "        #nominal_file_dsid = str(NOMINAL_FILE_TO_DSID[filename])\n",
    "        # Get the luminsotiy, DSID cross section, and per DSID total weighted events\n",
    "        #_scale_factor = luminosities_by_run[nominal_file_run_number] * NOMINAL_XSECTIONS[nominal_file_dsid] / NOMINAL_NORMALIZATIONS[nominal_file_dsid]\n",
    "        _scale_factor = 3378 * 1 / 1\n",
    "        # Extract the combined weight array\n",
    "        _weights = ak.concatenate(ak.unzip(weight_arr[weight_features][:, np.newaxis]), axis=1).to_numpy().prod(axis=1)\n",
    "\n",
    "        nominal_padded_jets = ak.fill_none(ak.pad_none(jet_arr[jet_features], max_jet_size, clip=True), 0, axis=1)\n",
    "        nominal_padded_electrons = ak.fill_none(ak.pad_none(electron_arr[electron_features], max_electron_size, clip=True), 0, axis=1)\n",
    "        nominal_padded_muons = ak.fill_none(ak.pad_none(muon_arr[muon_features], max_muon_size, clip=True), 0, axis=1)\n",
    "\n",
    "        current_data_size = len(_weights)\n",
    "\n",
    "        # put everything in train\n",
    "        if current_index + current_data_size < max_train_index:\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "        # put part in train and the rest in val\n",
    "        elif current_index < max_train_index and current_index + current_data_size < max_val_index:\n",
    "            split_index = max_train_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets, split_index=split_index, split_low=True)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons, split_index=split_index, split_low=True))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons, split_index=split_index, split_low=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put everything into val\n",
    "        elif current_index >= max_train_index and current_index + current_data_size < max_val_index:\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put part in val and the rest in test\n",
    "        elif current_index < max_val_index and current_index + current_data_size < max_test_index:\n",
    "            split_index = max_val_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets, split_index=split_index, split_low=True)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons, split_index=split_index, split_low=True))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons, split_index=split_index, split_low=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put everything into test\n",
    "        elif current_index >= max_val_index and current_index + current_data_size < max_test_index:\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put what's needed into test and ignore the rest\n",
    "        elif current_index < max_test_index and current_index + current_data_size >= max_test_index:\n",
    "            split_index = max_test_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, nominal_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, nominal_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, nominal_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        else:\n",
    "            print(\"Uncaught case:\",\n",
    "                  \"current_index: {}\".format(current_index),\n",
    "                  \"current_data_size: {}\".format(current_data_size),\n",
    "                  \"max_train_index: {}\".format(max_train_index),\n",
    "                  \"max_val_index: {}\".format(max_val_index),\n",
    "                  \"max_test_index: {}\".format(max_test_index), sep='\\n')\n",
    "\n",
    "        current_index += current_data_size\n",
    "        if current_index >= max_test_index:\n",
    "            break\n",
    "    if current_index >= max_test_index:\n",
    "        break\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate the alternative datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0006.root\n",
      "1347632\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0013.root\n",
      "1339381\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0007.root\n",
      "1383023\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0004.root\n",
      "1309020\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0010.root\n",
      "1374804\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0009.root\n",
      "1293776\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0000.root\n",
      "1310232\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0008.root\n",
      "1264826\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0003.root\n",
      "1327809\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0002.root\n",
      "1327524\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0005.root\n",
      "1332968\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0001.root\n",
      "1253316\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0012.root\n",
      "1262090\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0014.root\n",
      "913318\n",
      "Loading file: https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneEE5C_13TeV-powheg-herwigpp/cmsopendata2015_ttbar_19999_PU25nsData2015v1_76X_mcRun2_asymptotic_v12-v1_10000_0011.root\n",
      "1297345\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "df_train = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_training_data.root\")\n",
    "df_val = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_validation_data.root\")\n",
    "df_test = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_testing_data.root\")\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "current_index = 0\n",
    "for filename in all_variation_files:\n",
    "    print(\"Loading file: {}\".format(filename))\n",
    "    # Load the data\n",
    "    variation_dataset = uproot.open(filename)[\"Events\"]#.arrays(jet_features + electron_features + muon_features + weight_features)\n",
    "    filesize = int(variation_dataset.arrays(jet_features[0]).type.length)\n",
    "    print(filesize)\n",
    "    for i in range(int(np.ceil(filesize / chunk_size))):\n",
    "        jet_arr = variation_dataset.arrays(jet_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        electron_arr = variation_dataset.arrays(electron_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        muon_arr = variation_dataset.arrays(muon_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        weight_arr = variation_dataset.arrays(weight_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        # Get the run number\n",
    "        #variation_file_run_number = str(VARIATION_FILE_TO_RUN_NUMBER[filename])\n",
    "        # Get the DSID number\n",
    "        #variation_file_dsid = str(VARIATION_FILE_TO_DSID[filename])\n",
    "        # Get the luminsotiy, DSID cross section, and per DSID total weighted events\n",
    "        #_scale_factor = luminosities_by_run[variation_file_run_number] * VARIATION_XSECTIONS[variation_file_dsid] / VARIATION_NORMALIZATIONS[variation_file_dsid]\n",
    "        _scale_factor = 3378 * 1 / 1\n",
    "        # Extract the combined weight array\n",
    "        _weights = ak.concatenate(ak.unzip(weight_arr[weight_features][:, np.newaxis]), axis=1).to_numpy().prod(axis=1)\n",
    "\n",
    "        variation_padded_jets = ak.fill_none(ak.pad_none(jet_arr[jet_features], max_jet_size, clip=True), 0, axis=1)\n",
    "        variation_padded_electrons = ak.fill_none(ak.pad_none(electron_arr[electron_features], max_electron_size, clip=True), 0, axis=1)\n",
    "        variation_padded_muons = ak.fill_none(ak.pad_none(muon_arr[muon_features], max_muon_size, clip=True), 0, axis=1)\n",
    "\n",
    "        current_data_size = len(_weights)\n",
    "\n",
    "        # put everything in train\n",
    "        if current_index + current_data_size < max_train_index:\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "        # put part in train and the rest in val\n",
    "        elif current_index < max_train_index and current_index + current_data_size < max_val_index:\n",
    "            split_index = max_train_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets, split_index=split_index, split_low=True)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons, split_index=split_index, split_low=True))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons, split_index=split_index, split_low=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put everything into val\n",
    "        elif current_index >= max_train_index and current_index + current_data_size < max_val_index:\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put part in val and the rest in test\n",
    "        elif current_index < max_val_index and current_index + current_data_size < max_test_index:\n",
    "            split_index = max_val_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets, split_index=split_index, split_low=True)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons, split_index=split_index, split_low=True))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons, split_index=split_index, split_low=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put everything into test\n",
    "        elif current_index >= max_val_index and current_index + current_data_size < max_test_index:\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put what's needed into test and ignore the rest\n",
    "        elif current_index < max_test_index and current_index + current_data_size >= max_test_index:\n",
    "            split_index = max_test_index - current_index\n",
    "            data_dict = build_data_dict(jet_features, variation_padded_jets, split_index=split_index, split_high=True)\n",
    "            data_dict.update(build_data_dict(electron_features, variation_padded_electrons, split_index=split_index, split_high=True))\n",
    "            data_dict.update(build_data_dict(muon_features, variation_padded_muons, split_index=split_index, split_high=True))\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        else:\n",
    "            print(\"Uncaught case:\",\n",
    "                  \"current_index: {}\".format(current_index),\n",
    "                  \"current_data_size: {}\".format(current_data_size),\n",
    "                  \"max_train_index: {}\".format(max_train_index),\n",
    "                  \"max_val_index: {}\".format(max_val_index),\n",
    "                  \"max_test_index: {}\".format(max_test_index), sep='\\n')\n",
    "\n",
    "        current_index += current_data_size\n",
    "        if current_index >= max_test_index:\n",
    "            break\n",
    "    if current_index >= max_test_index:\n",
    "        break\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Now create simplified datasets for training the CARL models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_features = [jet_features[0] + \"{}\".format(i) for i in range(max_jet_size)]\n",
    "extended_features.extend([jet_features[1] + \"{}\".format(i) for i in range(max_jet_size)])\n",
    "extended_features.extend([jet_features[2] + \"{}\".format(i) for i in range(max_jet_size)])\n",
    "extended_features.extend([jet_features[3] + \"{}\".format(i) for i in range(max_jet_size)])\n",
    "\n",
    "extended_features.extend([electron_features[0] + \"{}\".format(i) for i in range(max_electron_size)])\n",
    "extended_features.extend([electron_features[1] + \"{}\".format(i) for i in range(max_electron_size)])\n",
    "extended_features.extend([electron_features[2] + \"{}\".format(i) for i in range(max_electron_size)])\n",
    "extended_features.extend([electron_features[3] + \"{}\".format(i) for i in range(max_electron_size)])\n",
    "\n",
    "extended_features.extend([muon_features[0] + \"{}\".format(i) for i in range(max_muon_size)])\n",
    "extended_features.extend([muon_features[1] + \"{}\".format(i) for i in range(max_muon_size)])\n",
    "extended_features.extend([muon_features[2] + \"{}\".format(i) for i in range(max_muon_size)])\n",
    "extended_features.extend([muon_features[3] + \"{}\".format(i) for i in range(max_muon_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_tree = uproot.open(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_training_data.root\")[\"Events\"]\n",
    "alternative_tree = uproot.open(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_training_data.root\")[\"Events\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11602238\n"
     ]
    }
   ],
   "source": [
    "df = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_training_data_CARL.root\")\n",
    "chunk_size = 100000\n",
    "\n",
    "# Load the data\n",
    "filesize = int(nominal_tree.arrays(jet_features[0]).type.length)\n",
    "print(filesize)\n",
    "\n",
    "weight_mean = ak.unzip(nominal_tree.arrays([\"weight_mc_combined\"]))[0].to_numpy().mean()\n",
    "for i in range(int(np.ceil(filesize / chunk_size))):\n",
    "    X_array = ak.concatenate(ak.unzip(nominal_tree.arrays(features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))), axis=1).to_numpy().astype(np.float32)\n",
    "    weight_array = ak.unzip(nominal_tree.arrays([\"weight_mc_combined\"], entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size)))[0].to_numpy()\n",
    "    # Normalize by the mean for training stability\n",
    "    weight_array /= weight_mean\n",
    "\n",
    "    data_dict = dict()\n",
    "    for ix, feat in enumerate(extended_features):\n",
    "        data_dict[feat] = X_array[:, ix].ravel()\n",
    "    data_dict[\"weight_mc_combined\"] = weight_array\n",
    "    fill_or_extend_tree(df, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11602238\n"
     ]
    }
   ],
   "source": [
    "df = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_training_data_CARL.root\")\n",
    "chunk_size = 100000\n",
    "\n",
    "# Load the data\n",
    "filesize = int(alternative_tree.arrays(jet_features[0]).type.length)\n",
    "print(filesize)\n",
    "\n",
    "weight_mean = ak.unzip(alternative_tree.arrays([\"weight_mc_combined\"]))[0].to_numpy().mean()\n",
    "for i in range(int(np.ceil(filesize / chunk_size))):\n",
    "    X_array = ak.concatenate(ak.unzip(alternative_tree.arrays(features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))), axis=1).to_numpy().astype(np.float32)\n",
    "    weight_array = ak.unzip(alternative_tree.arrays([\"weight_mc_combined\"], entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size)))[0].to_numpy()\n",
    "    # Normalize by the mean for training stability\n",
    "    weight_array /= weight_mean\n",
    "\n",
    "    data_dict = dict()\n",
    "    for ix, feat in enumerate(extended_features):\n",
    "        data_dict[feat] = X_array[:, ix].ravel()\n",
    "    data_dict[\"weight_mc_combined\"] = weight_array\n",
    "    fill_or_extend_tree(df, data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Making a DeepSets Dataset\n",
    "\n",
    "Best to have this as separate scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_DeepSets_training_data.root\")\n",
    "df_val = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_DeepSets_validation_data.root\")\n",
    "df_test = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_nominal_DeepSets_testing_data.root\")\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "current_index = 0\n",
    "for filename in all_nominal_files:\n",
    "    print(\"Loading file: {}\".format(filename))\n",
    "    # Load the data\n",
    "    nominal_dataset = uproot.open(filename)[\"Events\"]\n",
    "    filesize = int(nominal_dataset.arrays(jet_features[0]).type.length)\n",
    "    print(filesize)\n",
    "    for i in tqdm(range(int(np.ceil(filesize / chunk_size)))):\n",
    "        jet_arr = nominal_dataset.arrays(jet_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        electron_arr = nominal_dataset.arrays(electron_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        muon_arr = nominal_dataset.arrays(muon_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        weight_arr = nominal_dataset.arrays(weight_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        # Get the run number\n",
    "        #nominal_file_run_number = str(NOMINAL_FILE_TO_RUN_NUMBER[filename])\n",
    "        # Get the DSID number\n",
    "        #nominal_file_dsid = str(NOMINAL_FILE_TO_DSID[filename])\n",
    "        # Get the luminsotiy, DSID cross section, and per DSID total weighted events\n",
    "        #_scale_factor = luminosities_by_run[nominal_file_run_number] * NOMINAL_XSECTIONS[nominal_file_dsid] / NOMINAL_NORMALIZATIONS[nominal_file_dsid]\n",
    "        _scale_factor = 3378 * 1 / 1\n",
    "        # Extract the combined weight array\n",
    "        _weights = ak.concatenate(ak.unzip(weight_arr[weight_features][:, np.newaxis]), axis=1).to_numpy().prod(axis=1)\n",
    "\n",
    "        current_data_size = len(_weights)\n",
    "        \n",
    "        jet_sets = []\n",
    "        electron_sets = []\n",
    "        muon_sets = []\n",
    "        for j in range(current_data_size):\n",
    "            jet_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(jet_arr[jet_features][j])], axis=1).flatten()\n",
    "            )\n",
    "            electron_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(electron_arr[electron_features][j])], axis=1).flatten()\n",
    "            )\n",
    "            muon_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(muon_arr[muon_features][j])], axis=1).flatten()\n",
    "            )\n",
    "\n",
    "        nominal_features = [\"jet_4vec\", \"electron_4vec\", \"muon_4vec\"]\n",
    "        nominal_arr = ak.Array({\"jet_4vec\": jet_sets,\n",
    "                                \"electron_4vec\": electron_sets,\n",
    "                                \"muon_4vec\": muon_sets})\n",
    "\n",
    "        # put everything in train\n",
    "        if current_index + current_data_size < max_train_index:\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "        # put part in train and the rest in val\n",
    "        elif current_index < max_train_index and current_index + current_data_size < max_val_index:\n",
    "            split_index = max_train_index - current_index\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr, split_index=split_index, split_low=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put everything into val\n",
    "        elif current_index >= max_train_index and current_index + current_data_size < max_val_index:\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put part in val and the rest in test\n",
    "        elif current_index < max_val_index and current_index + current_data_size < max_test_index:\n",
    "            split_index = max_val_index - current_index\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr, split_index=split_index, split_low=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put everything into test\n",
    "        elif current_index >= max_val_index and current_index + current_data_size < max_test_index:\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put what's needed into test and ignore the rest\n",
    "        elif current_index < max_test_index and current_index + current_data_size >= max_test_index:\n",
    "            split_index = max_test_index - current_index\n",
    "            data_dict = build_data_dict(nominal_features, nominal_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        else:\n",
    "            print(\"Uncaught case:\",\n",
    "                  \"current_index: {}\".format(current_index),\n",
    "                  \"current_data_size: {}\".format(current_data_size),\n",
    "                  \"max_train_index: {}\".format(max_train_index),\n",
    "                  \"max_val_index: {}\".format(max_val_index),\n",
    "                  \"max_test_index: {}\".format(max_test_index), sep='\\n')\n",
    "\n",
    "        current_index += current_data_size\n",
    "        if current_index >= max_test_index:\n",
    "            break\n",
    "    if current_index >= max_test_index:\n",
    "        break\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_DeepSets_training_data.root\")\n",
    "df_val = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_DeepSets_validation_data.root\")\n",
    "df_test = uproot.recreate(\"/data/mdrnevich/AGC/CMS_ttbar_PS_var_DeepSets_testing_data.root\")\n",
    "\n",
    "chunk_size = 100000\n",
    "\n",
    "current_index = 0\n",
    "for filename in all_variation_files:\n",
    "    print(\"Loading file: {}\".format(filename))\n",
    "    # Load the data\n",
    "    variation_dataset = uproot.open(filename)[\"Events\"]\n",
    "    filesize = int(variation_dataset.arrays(jet_features[0]).type.length)\n",
    "    print(filesize)\n",
    "    for i in tqdm(range(int(np.ceil(filesize / chunk_size)))):\n",
    "        jet_arr = variation_dataset.arrays(jet_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        electron_arr = variation_dataset.arrays(electron_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        muon_arr = variation_dataset.arrays(muon_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        weight_arr = variation_dataset.arrays(weight_features, entry_start=int(i * chunk_size), entry_stop=int((i+1) * chunk_size))\n",
    "        # Get the run number\n",
    "        #variation_file_run_number = str(VARIATION_FILE_TO_RUN_NUMBER[filename])\n",
    "        # Get the DSID number\n",
    "        #variation_file_dsid = str(VARIATION_FILE_TO_DSID[filename])\n",
    "        # Get the luminsotiy, DSID cross section, and per DSID total weighted events\n",
    "        #_scale_factor = luminosities_by_run[variation_file_run_number] * VARIATION_XSECTIONS[variation_file_dsid] / VARIATION_NORMALIZATIONS[variation_file_dsid]\n",
    "        _scale_factor = 3378 * 1 / 1\n",
    "        # Extract the combined weight array\n",
    "        _weights = ak.concatenate(ak.unzip(weight_arr[weight_features][:, np.newaxis]), axis=1).to_numpy().prod(axis=1)\n",
    "\n",
    "        current_data_size = len(_weights)\n",
    "        \n",
    "        jet_sets = []\n",
    "        electron_sets = []\n",
    "        muon_sets = []\n",
    "        for i in range(current_data_size):\n",
    "            jet_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(jet_arr[jet_features][i])], axis=1).flatten()\n",
    "            )\n",
    "            electron_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(electron_arr[electron_features][i])], axis=1).flatten()\n",
    "            )\n",
    "            muon_sets.append(\n",
    "                np.concatenate([x.to_numpy()[:, np.newaxis] for x in ak.unzip(muon_arr[muon_features][i])], axis=1).flatten()\n",
    "            )\n",
    "\n",
    "        variation_features = [\"jet_4vec\", \"electron_4vec\", \"muon_4vec\"]\n",
    "        variation_arr = ak.Array({\"jet_4vec\": jet_sets,\n",
    "                                \"electron_4vec\": electron_sets,\n",
    "                                \"muon_4vec\": muon_sets})\n",
    "\n",
    "        # put everything in train\n",
    "        if current_index + current_data_size < max_train_index:\n",
    "            data_dict = build_data_dict(variation_features, variation_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "        # put part in train and the rest in val\n",
    "        elif current_index < max_train_index and current_index + current_data_size < max_val_index:\n",
    "            split_index = max_train_index - current_index\n",
    "            data_dict = build_data_dict(variation_features, variation_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_train, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(variation_features, variation_arr, split_index=split_index, split_low=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put everything into val\n",
    "        elif current_index >= max_train_index and current_index + current_data_size < max_val_index:\n",
    "            data_dict = build_data_dict(variation_features, variation_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "        # put part in val and the rest in test\n",
    "        elif current_index < max_val_index and current_index + current_data_size < max_test_index:\n",
    "            split_index = max_val_index - current_index\n",
    "            data_dict = build_data_dict(variation_features, variation_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_val, data_dict)\n",
    "\n",
    "            data_dict = build_data_dict(variation_features, variation_arr, split_index=split_index, split_low=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[split_index:] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put everything into test\n",
    "        elif current_index >= max_val_index and current_index + current_data_size < max_test_index:\n",
    "            data_dict = build_data_dict(variation_features, variation_arr)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        # put what's needed into test and ignore the rest\n",
    "        elif current_index < max_test_index and current_index + current_data_size >= max_test_index:\n",
    "            split_index = max_test_index - current_index\n",
    "            data_dict = build_data_dict(variation_features, variation_arr, split_index=split_index, split_high=True)\n",
    "            data_dict[\"weight_mc_combined\"] = _weights[:split_index] * _scale_factor\n",
    "            fill_or_extend_tree(df_test, data_dict)\n",
    "        else:\n",
    "            print(\"Uncaught case:\",\n",
    "                  \"current_index: {}\".format(current_index),\n",
    "                  \"current_data_size: {}\".format(current_data_size),\n",
    "                  \"max_train_index: {}\".format(max_train_index),\n",
    "                  \"max_val_index: {}\".format(max_val_index),\n",
    "                  \"max_test_index: {}\".format(max_test_index), sep='\\n')\n",
    "\n",
    "        current_index += current_data_size\n",
    "        if current_index >= max_test_index:\n",
    "            break\n",
    "    if current_index >= max_test_index:\n",
    "        break\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carl-for-agc",
   "language": "python",
   "name": "carl-for-agc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
